{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrega Final - Grupo 12\n",
    "***\n",
    "\n",
    "*Introducción a la Minería de Datos - Otoño 2018*\n",
    "\n",
    "Pedro Belmonte,  \n",
    "Jorge Fabry,  \n",
    "Víctor Garrido,  \n",
    "Pablo Ilabaca\n",
    "\n",
    "__[GitHub](https://github.com/VictorPato/mineria-de-datos-entrega-3)__\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivación\n",
    "***\n",
    "La industria de los videojuegos es una de las más grandes industrias de entretenimiento a nivel mundial. Las mayores publicaciones se enfrentan codo a codo para conseguir el mayor éxito y con esto mayores ventas.\n",
    "\n",
    "En esta industria, como en muchas otras, los críticos juegan un rol vital a la hora de definir la recepción que tendrá un juego. Casi siempre, los críticos reciben copias de juegos antes de que estos sean lanzados al público, por lo que tienen la primera palabra a la hora de publicitar si un juego es de calidad o no.\n",
    "\n",
    "Dado esto, se da origen a un fenómeno en el que los críticos dan muy buena crítica a un juego, tal vez motivados por dinero o por quedar bien con los publicadores para seguir recibiendo acceso exclusivo a los juegos, y luego los usuarios dan un puntaje mucho menor, dejando una sensación de engaño y desencanto. A estos juegos con una gran diferencia de puntaje los llamaremos **fiascos**.\n",
    "\n",
    "<img src=\"media/intro-ejemplos_de_fiascos.PNG\" alt=\"Ejemplos de Fiascos\" title=\"Puntaje de un par de fiascos en Metacritic\" />\n",
    "\n",
    "Interesa entonces utilizar las herramientas que provee este curso para estudiar los distintos patrones que pueden surgir a la hora de puntuar la calidad de un juego. En específico,  lograr crear un _predictor_ para saber, ojalá con bastante seguridad, si un juego será un **fiasco** o no.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set\n",
    "***\n",
    "## Origen\n",
    "Para explorar el fenómeno antes descrito, se utiliza el dataset extraido de https://www.kaggle.com/silver1996/videogames/data.\n",
    "\n",
    "Este se construye de datos de Metacritic.com, el cual incluye 16719 entradas con los datos que se presentan a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "original_data = pd.read_csv('data/Video_Games_Sales_as_at_22_Dec_2016.csv',encoding='latin1')\n",
    "print(\"(Filas x Columnas) = \",original_data.shape)\n",
    "original_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los gráficos presentados a continuación se contruyen con este data set, con la intención de extraer información útil de este.\n",
    "\n",
    "<img src=\"media/datos-publicadores_controversiales.jpg\" alt=\"Publicadores controversiales\" title=\"Publicadores con mayor diferencia de puntajes entre críticos y usuarios\" />\n",
    "<img src=\"media/datos-puntaje_por_genero.png\" alt=\"Puntajes por género\" title=\"Puntaje por género otorgado por los críticos de IGN\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que el data set incluye muchas columnas con distintas variables, y también algunas filas que le faltan valores. Para comenzar a usar este data set se debe hacer una limpieza que solo deje datos que sean útiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza\n",
    "El siguiente script en R busca limpiar y ordenar el data set para ser utilizado posteriormente por los clasificadores. Se borran varias columnas que no estarían disponibles cuando sale un juego, como ventas. También se borran columnas que no nos aportarán información al explorar a futuro, como el año de lanzamiento, o el nombre del juego.\n",
    "\n",
    "Se busca tambien limitar la cantidad de valores distintos de varias columnas, para mantener el problema con baja dimensionalidad."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "########### 1 - creacion de la tabla y tipos ################\n",
    "## cambiar esta linea al path del .csv\n",
    "## si se mantiene la estructura de carpetas del repositorio, basta con \n",
    "## dejar el working directory en la ubicacion de este archivo\n",
    "library(readr)\n",
    "videogames <- read_csv(\"../data/Video_Games_Sales_as_at_22_Dec_2016.csv\")\n",
    "\n",
    "# se eliminan las filas que tengan NA (quedan aprox 7000 resultados)\n",
    "filtered_vid = na.omit(videogames)\n",
    "\n",
    "# se arreglan los tipos: user-score a numeric, los otros char a factor\n",
    "filtered_vid$User_Score <- as.numeric(filtered_vid$User_Score)\n",
    "character_vars <- lapply(filtered_vid, class) == \"character\"\n",
    "filtered_vid[, character_vars] <- lapply(filtered_vid[, character_vars], as.factor)\n",
    "\n",
    "# se define la diferencia necesaria para considerar que un juego fue un fiasco\n",
    "# (al comparar user_score con critic_score)\n",
    "dif_for_fail = 2\n",
    "\n",
    "# se crea y pobla la columna que define si un juego es un fiasco o no\n",
    "filtered_vid[\"Is_Fiasco\"] <- NA\n",
    "filtered_vid$Is_Fiasco <- ((filtered_vid$Critic_Score / 10) - filtered_vid$User_Score) > dif_for_fail\n",
    "\n",
    "########## 2 - reduccion de dimensionalidad para Machine Learning ##########\n",
    "## eliminar categorias\n",
    "reduced <- filtered_vid\n",
    "reduced[\"User_Count\"] <- NULL\n",
    "reduced[\"User_Score\"] <- NULL\n",
    "reduced[\"Critic_Count\"] <- NULL\n",
    "reduced[\"Other_Sales\"] <- NULL\n",
    "reduced[\"EU_Sales\"] <- NULL\n",
    "reduced[\"JP_Sales\"] <- NULL\n",
    "reduced[\"NA_Sales\"] <- NULL\n",
    "reduced[\"Year_of_Release\"] <- NULL\n",
    "reduced[\"Name\"] <- NULL\n",
    "reduced[\"Developer\"] <- NULL\n",
    "reduced[\"Developer\"] <- NULL\n",
    "## tomar solo las n consolas mas populares:\n",
    "n = 5\n",
    "popular_console <- reduced\n",
    "popular_console[\"counter\"] <- 1\n",
    "popular_console = aggregate(counter ~ Platform,popular_console,FUN=sum)\n",
    "popular_console = popular_console[order(popular_console$counter,decreasing = T),]\n",
    "reduced <- reduced[reduced$Platform %in% popular_console[1:n,]$Platform,]\n",
    "\n",
    "## tomar solo los n mayores publicadores:\n",
    "n = 10\n",
    "popular_publisher <- reduced\n",
    "popular_publisher[\"counter\"] <- 1\n",
    "popular_publisher = aggregate(counter ~ Publisher,popular_publisher,FUN=sum)\n",
    "popular_publisher = popular_publisher[order(popular_publisher$counter,decreasing = T),]\n",
    "reduced <- reduced[reduced$Publisher %in% popular_publisher[1:n,]$Publisher,]\n",
    "#### Descomentar siguiente linea para exportar el dataset\n",
    "write.csv(reduced, file = \"../data/data_para_clasificadores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tras esto, se tienen 8 columnas. La primera un número por cada juego. Las siguientes 6 son parámetros, y la última la clase a clasificiar del juego. La clase corresponde a si el juego es un fiasco o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/data_para_clasificadores.csv',encoding='latin1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe notar que las clases **no** están balanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Cantidad de Fiascos\")\n",
    "data['Is_Fiasco'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaptando el Data Set para clasificadores\n",
    "Los clasificadores no pueden trabajar con Strings directamente. Vemos que Platform, Genre, Publisher y Rating son categorías que utilizan Strings, y hay que aplicar algún tipo de transformación para poder alimentarlas al clasificador.\n",
    "\n",
    "Para esto, se utiliza un LabelBinarizer, que permite covertir las distintas categorías de una columna en columnas independientes. El resultado final es una matriz de dimensiones: 2348 rows x 34 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "## Se aplica LabelBinarizer columna por columna, y finalmente se unen los resultados\n",
    "## En header se van guardando los nombres de cada columna para luego agregarlas al nuevo Data Set\n",
    "lb = preprocessing.LabelBinarizer()\n",
    "\n",
    "lb.fit(data[\"Platform\"])\n",
    "platform = lb.transform(data[\"Platform\"])\n",
    "header = lb.classes_\n",
    "\n",
    "lb.fit(data[\"Genre\"])\n",
    "genre = lb.transform(data[\"Genre\"])\n",
    "header = np.append(header,lb.classes_)\n",
    "\n",
    "lb.fit(data[\"Publisher\"])\n",
    "publisher = lb.transform(data[\"Publisher\"])\n",
    "header = np.append(header,lb.classes_)\n",
    "\n",
    "##sales = np.transpose(np.matrix(data[\"Global_Sales\"].values))\n",
    "##header = np.append(header,\"Global_Sales\")\n",
    "\n",
    "critic_score = np.transpose(np.matrix(data[\"Critic_Score\"].values))\n",
    "header = np.append(header,\"Critic_Score\")\n",
    "\n",
    "lb.fit(data[\"Rating\"])\n",
    "rating = lb.transform(data[\"Rating\"])\n",
    "header = np.append(header,lb.classes_)\n",
    "\n",
    "fiasco = np.transpose(np.matrix(data[\"Is_Fiasco\"].values))\n",
    "header = np.append(header,\"Is_Fiasco\")\n",
    "\n",
    "new_matrix = np.hstack((platform,genre,publisher,critic_score,rating,fiasco))\n",
    "new_data = pd.DataFrame(new_matrix)\n",
    "new_data.columns = header\n",
    "\n",
    "## Se separa los datos de los resultados a predecir.\n",
    "X = new_data[new_data.columns[:-1]]\n",
    "y = new_data[new_data.columns[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para observar la nueva data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encontrando nuestro Predictor\n",
    "***\n",
    "## Experimentos básicos para elegir predictor\n",
    "Como _predictor de fiascos_, se busca tener un clasificador que tenga un porcentaje alto de predicción de fiascos. Mediante varios experimentos, se muestra a continuación como se comparan varios clasificadores ante nuestro data set.\n",
    "\n",
    "Utilizando código del laboratorio 2.2 del curso, se comparan distintos clasificadores mediante el contraste de las métricas promedio obtenidas tras un buen número de pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import graphviz\n",
    "import io\n",
    "import pydotplus\n",
    "import imageio\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB  # naive bayes\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC  # support vector machine classifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clf_with_cross_val(clf, X, y, num_tests=100, k=5):\n",
    "    metrics = {'f1-score': [], 'precision': [], 'recall': [], 'score': []}\n",
    "    \n",
    "    for _ in range(num_tests):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.30, stratify=y)\n",
    "        clf.fit(X_train, y_train)\n",
    "        predictions = clf.predict(X_test)\n",
    "        scores = cross_val_score(clf, X, y, cv=k, scoring='f1')\n",
    "        \n",
    "        metrics['f1-score'].append(f1_score(y_test,predictions))\n",
    "        metrics['recall'].append(recall_score(y_test,predictions))\n",
    "        metrics['precision'].append(precision_score(y_test,predictions))\n",
    "        metrics['score'].append(scores.mean())\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "def run_many_classifiers(X, y, num_test):\n",
    "    c0 = (\"Base Dummy\", DummyClassifier(strategy='stratified'))\n",
    "    c1 = (\"Decision Tree\", DecisionTreeClassifier(min_samples_split=100))\n",
    "    c2 = (\"Gaussian Naive Bayes\", GaussianNB())\n",
    "    c3 = (\"KNN-3\", KNeighborsClassifier(n_neighbors=3))\n",
    "    c4 = (\"KNN-5\", KNeighborsClassifier(n_neighbors=5))\n",
    "    c5 = (\"Random Forest\",RandomForestClassifier(max_features=\"auto\", max_depth=15, n_estimators=40))\n",
    "\n",
    "\n",
    "    classifiers = [c0, c1, c2, c3, c4, c5]\n",
    "    print(\"Corriendo \"+ str(num_test) + \" tests por clasificador\\n\")\n",
    "\n",
    "    for name, clf in classifiers:\n",
    "        metrics = run_clf_with_cross_val(clf, X, y, num_test)\n",
    "        print(\"----------------\")\n",
    "        print(\"Resultados para clasificador: \",name) \n",
    "        print(\"Precision promedio:\",np.array(metrics['precision']).mean())\n",
    "        print(\"Recall promedio:\",np.array(metrics['recall']).mean())\n",
    "        print(\"F1-score promedio:\",np.array(metrics['f1-score']).mean())\n",
    "        print(\"Cross Validation F1-score promedio:\", np.array(metrics['score']).mean())\n",
    "        \n",
    "run_many_classifiers(X, y, 20)\n",
    "warnings.filterwarnings('once')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que los resultados obtenidos no son considerablemente buenos. Tomando en cuenta el F1-score, que describe en general la eficacia de un clasificador, los puntajes son bien bajos, aunque aún así mejores que el base dummy. \n",
    "\n",
    "De todos los clasificadores explorados, Gaussan Naive Bayes y Random Forest son los que se ven más prometedores a predictor de fiascos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando Subsampling y Oversampling\n",
    "En un intento de encontrar mejores resultados que los anteriores, se aplicarán estas estrategias sobre el dataset, buscando que los clasificadores aprendan mejor teniendo clases balanceadas.\n",
    "Nuevamente nos apoyamos en código trabajado en el laboratorio 2.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling sobre la clase True\n",
    "idx = np.random.choice(new_data.loc[data.Is_Fiasco == True].index, size=1990)\n",
    "data_oversampled = pd.concat([new_data, new_data.iloc[idx]])\n",
    "\n",
    "print(\"Data oversampled on class 'True'\")\n",
    "print(data_oversampled['Is_Fiasco'].value_counts())\n",
    "print()\n",
    "\n",
    "# subsampling sobre la clase False\n",
    "idx = np.random.choice(new_data.loc[new_data.Is_Fiasco == False].index, size=1990, replace=False)\n",
    "data_subsampled = new_data.drop(new_data.iloc[idx].index)\n",
    "\n",
    "print(\"Data subsampled on class 'False'\")\n",
    "print(data_subsampled['Is_Fiasco'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# datos \"oversampleados\" \n",
    "X_over = data_oversampled[new_data.columns[:-1]]\n",
    "y_over = data_oversampled[new_data.columns[-1]]\n",
    "\n",
    "# datos \"subsampleados\"\n",
    "X_subs = data_subsampled[new_data.columns[:-1]]\n",
    "y_subs = data_subsampled[new_data.columns[-1]]\n",
    "\n",
    "print(\"----------Prueba Oversampling------------\")\n",
    "run_many_classifiers(X_over, y_over, 20)\n",
    "\n",
    "print(\"\\n\\n----------Prueba Subsampling------------\")\n",
    "run_many_classifiers(X_subs, y_subs, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También, como experimento, se buscó hacer subsampling y oversampling al mismo tiempo para no repetir tantos datos, pero tampoco quedarnos con tan pocos. Esto se muestra a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "idx = np.random.choice(new_data.loc[data.Is_Fiasco == True].index, size=71)\n",
    "data_master = pd.concat([new_data, new_data.iloc[idx]])\n",
    "idx = np.random.choice(new_data.loc[new_data.Is_Fiasco == False].index, size=1669, replace=False)\n",
    "data_master = data_master.drop(new_data.iloc[idx].index)\n",
    "print(\"Data subsampled on class 'False' and oversampled on class 'True'\")\n",
    "print(data_master['Is_Fiasco'].value_counts())\n",
    "X_mast = data_master[new_data.columns[:-1]]\n",
    "y_mast = data_master[new_data.columns[-1]]\n",
    "run_many_classifiers(X_mast, y_mast, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gráficos de resultados obtenidos\n",
    "A continuación se grafican los resultados obtenidos para Base Dummy, Decision Tree y Random Forest. Como se han corrido varias veces los clasificadores puede que los gráficos no correspondan perfectamente a los valores, pero sí con la cercanía suficiente para ser precisos.\n",
    "<img src=\"media/clasificadores-datos_crudos.png\" alt=\"\" title=\"\" />\n",
    "<img src=\"media/clasificadores-datos_oversampling.png\" alt=\"a\" title=\"a\" />\n",
    "<img src=\"media/clasificadores-datos_subsampling.png\" alt=\"\" title=\"\" />\n",
    "<img src=\"media/clasificadores-datos_over_y_sub.png\" alt=\"\" title=\"\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión Clasificadores\n",
    "Haciendo un análisis de los resultados obtenidos, podemos considerar que **Random Forest** es clasificador que tiene mejor desempeño en cuanto a resultados, en especial al hacer **oversampling**.\n",
    "\n",
    "Si bien no es siempre certero, tiene un puntaje suficiente de exactitud, lo que consideramos un logro aceptable con respecto a lo que esperabamos obtener.\n",
    "Si necesitaramos crear un predictor efectivo, utilizariamos ese clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graficando Decision Trees\n",
    "En un ejercicio para explorar la importancia de las variables, y para observar la lógica de los decision trees, se grafican los árboles de decisión, donde los colores indican afinidad con una clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c12= DecisionTreeClassifier(min_samples_split=100)\n",
    "c13= DecisionTreeClassifier(min_samples_split=100)\n",
    "c14= DecisionTreeClassifier(min_samples_split=100)\n",
    "c15= DecisionTreeClassifier(min_samples_split=100)\n",
    "features=new_data.columns[:-1]\n",
    "train, test= train_test_split(new_data,test_size=.30, stratify=y)\n",
    "\n",
    "        \n",
    "X_train= train[features]\n",
    "y_train=train[\"Is_Fiasco\"]\n",
    "\n",
    "X_test=test[features]\n",
    "y_test=test[\"Is_Fiasco\"]\n",
    "\n",
    "dt12=c12.fit(X_train,y_train)\n",
    "dt13=c13.fit(X_over,y_over)\n",
    "dt14=c14.fit(X_subs,y_subs)\n",
    "dt15=c15.fit(X_mast,y_mast)\n",
    "\n",
    "def show_tree(tree, features, path):\n",
    "    path = \"images/\" + path\n",
    "    f= io.StringIO()\n",
    "    export_graphviz(tree, out_file=f, feature_names=features,filled=True,rounded=True)\n",
    "    pydotplus.graph_from_dot_data(f.getvalue()).write_png(path)\n",
    "    img= imageio.imread(path)\n",
    "    plt.rcParams[\"figure.figsize\"]=(20,20)\n",
    "    plt.imshow(img)\n",
    "    \n",
    "#show_tree(dt12, features, 'arbol_normal.png')\n",
    "#show_tree(dt13, features, 'arbol_oversampled.png')\n",
    "#show_tree(dt14, features, 'arbol_subsampled.png')\n",
    "#show_tree(dt15, features, 'arbol_master.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árbol Normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(dt12, features, 'arbol_normal.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árbol Oversampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(dt13, features, 'arbol_oversampled.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árbol Subsampled:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(dt14, features, 'arbol_subsampled.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Árbol con Oversampling y Subsampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_tree(dt15, features, 'arbol_master.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importancia de Atributos\n",
    "Aplicando Random Forest, se pueden obtener los atributos que más seguido se utilizan para discriminar la clase a clasificar. De esta forma se puede evidenciar cuales son las columnas más importantes para decidir si un juego es un fiasco o no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "clf6 = RandomForestClassifier(n_estimators= 1000,max_depth=100, random_state=0)\n",
    "train, test= train_test_split(new_data,test_size=.30, stratify=y)\n",
    "        \n",
    "X_train= train[features]\n",
    "y_train=train[\"Is_Fiasco\"]\n",
    "\n",
    "X_test=test[features]\n",
    "y_test=test[\"Is_Fiasco\"]\n",
    "clf6.fit(X_train, y_train)\n",
    "mi_lista_de_tuplas = []\n",
    "for i in range(33):\n",
    "    tupla = (header[i],clf6.feature_importances_[i])\n",
    "    mi_lista_de_tuplas.append(tupla)\n",
    "mi_lista_de_tuplas.sort(key=operator.itemgetter(1), reverse=True)\n",
    "for i in range(33):\n",
    "    print(mi_lista_de_tuplas[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos con esto que la variable más importante por lejos es Critic_Score, lo cual tiene mucho sentido pues con esta en parte se define la clase Is_Fiasco. Luego, dentro de las más importantes tenemos una plataforma (PC), dos publicadores (Activision y EA) y dos géneros (Action y Sports)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
